from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from openai import OpenAI
import os
from dotenv import load_dotenv
from typing import List, Optional, Dict, Any
import json
from enum import Enum
import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Initialize FastAPI app
app = FastAPI(title="Course Management API")

# Initialize OpenAI client
def get_openai_client():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("OpenAI API key not configured")
        raise HTTPException(status_code=500, detail="OpenAI API key not configured")
    return OpenAI(api_key=api_key)

# ==================== ENUMS ====================

class ExamStatus(str, Enum):
    ENABLED = "Enabled"
    DISABLED = "Disabled"

class AssessmentType(str, Enum):
    QUIZ = "Quiz"
    ASSIGNMENT = "Assignment"
    MIDTERM = "Midterm"
    FINAL = "Final"
    PROJECT = "Project"

class DifficultyLevel(str, Enum):
    BEGINNER = "Beginner"
    INTERMEDIATE = "Intermediate"
    ADVANCED = "Advanced"

# ==================== DIAGNOSTIC TEST MODELS ====================

class DiagnosticTestRequest(BaseModel):
    course_name: str = Field(..., description="Name of the course")
    subject: str = Field(..., description="Subject area")
    target_grade_level: str = Field(..., description="Target grade or level")
    number_of_mcq: int = Field(..., ge=1, le=50, description="Number of MCQ questions (1-50)")

class MCQOption(BaseModel):
    option: str
    text: str

class MCQQuestion(BaseModel):
    question_number: int
    question: str
    options: List[MCQOption]
    correct_answer: str
    explanation: str

class DiagnosticTestResponse(BaseModel):
    course_name: str
    subject: str
    target_grade_level: str
    total_questions: int
    questions: List[MCQQuestion]

# ==================== ENHANCED SYLLABUS MODELS ====================

class SyllabusRequest(BaseModel):
    course_name: str = Field(..., description="Name of the course", example="English 101")
    subject: str = Field(..., description="Subject area", example="English")
    target_grade_level: str = Field(..., description="Target grade or level", example="Grade 9")
    course_length: str = Field(..., description="Course duration", example="Full Year")
    semester_count: int = Field(..., ge=1, le=4, description="Number of semesters (1-4)", example=3)
    quizzes_per_module: int = Field(..., ge=0, le=10, description="Number of quizzes per module", example=3)
    midterm_examination: ExamStatus = Field(..., description="Midterm exam status")
    final_examination: ExamStatus = Field(..., description="Final exam status")
    total_modules: int = Field(..., ge=1, le=20, description="Total number of modules", example=5)

class ContentSpecification(BaseModel):
    """Specification for text content to be generated by AI"""
    content_id: str = Field(..., description="Unique identifier for this content piece")
    title: str = Field(..., description="Title of the content")
    description: str = Field(..., description="Brief description of what this content covers")
    estimated_reading_time_minutes: int = Field(..., ge=5, le=10, description="Reading time in minutes (5-10)")
    word_count_estimate: int = Field(..., description="Estimated word count (based on 200 words/min reading speed)")
    key_points: List[str] = Field(..., description="Key points that the content should cover")
    difficulty_level: DifficultyLevel = Field(..., description="Difficulty level")

class LearningOutcome(BaseModel):
    outcome_id: str = Field(..., description="Unique identifier")
    description: str = Field(..., description="What students will achieve")
    bloom_level: str = Field(..., description="Bloom's taxonomy level")

class SubTopic(BaseModel):
    subtopic_id: str = Field(..., description="Unique identifier")
    title: str = Field(..., description="Subtopic title")
    description: str = Field(..., description="Detailed description")
    key_concepts: List[str] = Field(..., description="Key concepts")
    content_specification: ContentSpecification = Field(..., description="Specification for AI-generated text content")
    difficulty_level: DifficultyLevel = Field(..., description="Difficulty level")

class Topic(BaseModel):
    topic_id: str = Field(..., description="Unique identifier")
    title: str = Field(..., description="Topic title")
    description: str = Field(..., description="Detailed description")
    subtopics: List[SubTopic] = Field(..., description="List of subtopics")
    prerequisites: List[str] = Field(default=[], description="Prerequisites")
    total_reading_time_minutes: int = Field(..., description="Total reading time for all subtopics")

class QuizSpecification(BaseModel):
    quiz_id: str = Field(..., description="Unique identifier")
    quiz_number: int = Field(..., description="Quiz number")
    title: str = Field(..., description="Quiz title")
    description: str = Field(..., description="Quiz description")
    topics_covered: List[str] = Field(..., description="Topic IDs covered")
    question_types: Dict[str, int] = Field(..., description="Question types and counts")
    total_questions: int = Field(..., description="Total questions")
    duration_minutes: int = Field(..., description="Time allowed")
    difficulty_distribution: Dict[str, int] = Field(..., description="Difficulty distribution")
    passing_score_percentage: int = Field(..., description="Passing score")
    weight_percentage: float = Field(..., description="Weight in grade")

class AssignmentSpecification(BaseModel):
    assignment_id: str = Field(..., description="Unique identifier")
    title: str = Field(..., description="Assignment title")
    description: str = Field(..., description="Assignment description")
    type: str = Field(..., description="Assignment type")
    topics_covered: List[str] = Field(..., description="Topics covered")
    requirements: List[str] = Field(..., description="Requirements")
    deliverables: List[str] = Field(..., description="Deliverables")
    grading_criteria: List[Dict[str, str]] = Field(..., description="Grading criteria")
    estimated_work_hours: float = Field(..., description="Work hours")
    weight_percentage: float = Field(..., description="Weight in grade")

class Module(BaseModel):
    module_id: str = Field(..., description="Unique identifier")
    module_number: int = Field(..., description="Module number")
    title: str = Field(..., description="Module title")
    description: str = Field(..., description="Module description")
    learning_outcomes: List[LearningOutcome] = Field(..., description="Learning outcomes")
    topics: List[Topic] = Field(..., description="Topics")
    quizzes: List[QuizSpecification] = Field(..., description="Quizzes")
    assignments: List[AssignmentSpecification] = Field(default=[], description="Assignments")
    duration_weeks: int = Field(..., description="Duration in weeks")
    total_reading_time_minutes: int = Field(..., description="Total reading time for all content")
    prerequisites: List[str] = Field(default=[], description="Prerequisites")
    skills_developed: List[str] = Field(..., description="Skills developed")

class ExamSpecification(BaseModel):
    exam_id: str = Field(..., description="Unique identifier")
    title: str = Field(..., description="Exam title")
    type: AssessmentType = Field(..., description="Exam type")
    coverage: List[str] = Field(..., description="Modules covered")
    format: str = Field(..., description="Exam format")
    question_types: Dict[str, int] = Field(..., description="Question types")
    total_questions: int = Field(..., description="Total questions")
    duration_minutes: int = Field(..., description="Duration")
    difficulty_distribution: Dict[str, int] = Field(..., description="Difficulty distribution")
    topics_breakdown: List[Dict[str, Any]] = Field(..., description="Topics breakdown")
    passing_score_percentage: int = Field(..., description="Passing score")
    weight_percentage: float = Field(..., description="Weight in grade")
    allowed_materials: List[str] = Field(default=[], description="Allowed materials")
    instructions: List[str] = Field(..., description="Instructions")

class Semester(BaseModel):
    semester_id: str = Field(..., description="Unique identifier")
    semester_number: int = Field(..., description="Semester number")
    title: str = Field(..., description="Semester title")
    description: str = Field(..., description="Semester overview")
    modules: List[Module] = Field(..., description="Modules")
    midterm_exam: Optional[ExamSpecification] = Field(None, description="Midterm exam")
    final_exam: Optional[ExamSpecification] = Field(None, description="Final exam")
    total_weeks: int = Field(..., description="Total weeks")
    semester_learning_outcomes: List[str] = Field(..., description="Learning outcomes")

class GradingBreakdown(BaseModel):
    quizzes_percentage: float = Field(..., description="Quizzes weight")
    assignments_percentage: float = Field(..., description="Assignments weight")
    midterm_percentage: float = Field(default=0, description="Midterm weight")
    final_percentage: float = Field(default=0, description="Final weight")
    participation_percentage: float = Field(default=0, description="Participation weight")
    projects_percentage: float = Field(default=0, description="Projects weight")

class AssessmentSummary(BaseModel):
    total_quizzes: int = Field(..., description="Total quizzes")
    total_assignments: int = Field(..., description="Total assignments")
    midterm_enabled: bool = Field(..., description="Midterm enabled")
    final_enabled: bool = Field(..., description="Final enabled")
    grading_breakdown: GradingBreakdown = Field(..., description="Grading breakdown")
    grading_scale: Dict[str, str] = Field(..., description="Grading scale")

class CourseMetadata(BaseModel):
    course_code: str = Field(..., description="Course code")
    credits: int = Field(..., description="Credit hours")
    prerequisites: List[str] = Field(default=[], description="Prerequisites")
    corequisites: List[str] = Field(default=[], description="Corequisites")
    required_materials: List[str] = Field(..., description="Required materials")
    recommended_materials: List[str] = Field(default=[], description="Recommended materials")
    learning_platform: str = Field(default="Online", description="Learning platform")

class SyllabusResponse(BaseModel):
    syllabus_id: str = Field(..., description="Unique identifier")
    course_metadata: CourseMetadata = Field(..., description="Course metadata")
    course_name: str = Field(..., description="Course name")
    subject: str = Field(..., description="Subject")
    target_grade_level: str = Field(..., description="Grade level")
    course_length: str = Field(..., description="Course length")
    total_modules: int = Field(..., description="Total modules")
    total_semesters: int = Field(..., description="Total semesters")
    course_description: str = Field(..., description="Course description")
    course_objectives: List[str] = Field(..., description="Course objectives")
    semesters: List[Semester] = Field(..., description="Semesters")
    assessment_summary: AssessmentSummary = Field(..., description="Assessment summary")
    total_content_reading_time_minutes: int = Field(..., description="Total reading time for all content")

# ==================== HELPER FUNCTIONS ====================

def generate_module(client: OpenAI, request: SyllabusRequest, module_number: int, total_modules: int) -> Dict:
    """Generate a single module with all its details"""
    logger.info(f"Generating module {module_number}/{total_modules} for {request.course_name}")
    
    # Calculate reading time: Average 200 words per minute, 5-10 minutes per subtopic
    min_reading_time = 5
    max_reading_time = 10
    avg_reading_time = 7
    words_per_minute = 200
    
    prompt = f"""Generate a detailed module for a {request.subject} course.

COURSE CONTEXT:
- Course: {request.course_name}
- Grade Level: {request.target_grade_level}
- Module Number: {module_number} of {total_modules}
- Quizzes Required: {request.quizzes_per_module}

CONTENT REQUIREMENTS:
- Each subtopic must have TEXT CONTENT ONLY (no videos, no external resources)
- Reading time per subtopic: {min_reading_time}-{max_reading_time} minutes
- Word count per subtopic: {min_reading_time * words_per_minute}-{max_reading_time * words_per_minute} words
- All content will be generated by AI later based on specifications

GENERATE module {module_number} with:
- 2-3 main topics, each with 2-3 subtopics
- Content specifications for each subtopic (what the AI should write about)
- {request.quizzes_per_module} quizzes covering different topics
- 1 assignment

Return ONLY valid JSON matching this structure:
{{
  "module_id": "mod_{module_number}",
  "module_number": {module_number},
  "title": "Module title relevant to {request.subject}",
  "description": "Detailed description",
  "duration_weeks": 2,
  "total_reading_time_minutes": 0,
  "prerequisites": [],
  "skills_developed": ["skill1", "skill2"],
  "learning_outcomes": [
    {{
      "outcome_id": "lo_{module_number}_1",
      "description": "Students will be able to...",
      "bloom_level": "Apply"
    }}
  ],
  "topics": [
    {{
      "topic_id": "topic_{module_number}_1",
      "title": "Topic title",
      "description": "Topic description",
      "total_reading_time_minutes": 0,
      "prerequisites": [],
      "subtopics": [
        {{
          "subtopic_id": "subtopic_{module_number}_1_1",
          "title": "Subtopic title",
          "description": "Subtopic description",
          "key_concepts": ["concept1", "concept2", "concept3"],
          "difficulty_level": "Beginner",
          "content_specification": {{
            "content_id": "content_{module_number}_1_1",
            "title": "Content title",
            "description": "What this text content will cover in detail",
            "estimated_reading_time_minutes": {avg_reading_time},
            "word_count_estimate": {avg_reading_time * words_per_minute},
            "key_points": [
              "First key point to cover",
              "Second key point to cover",
              "Third key point to cover"
            ],
            "difficulty_level": "Beginner"
          }}
        }}
      ]
    }}
  ],
  "quizzes": [
    {{
      "quiz_id": "quiz_{module_number}_1",
      "quiz_number": 1,
      "title": "Quiz title",
      "description": "Quiz description",
      "topics_covered": ["topic_{module_number}_1"],
      "question_types": {{"mcq": 10, "short_answer": 2}},
      "total_questions": 12,
      "duration_minutes": 20,
      "difficulty_distribution": {{"easy": 5, "medium": 5, "hard": 2}},
      "passing_score_percentage": 70,
      "weight_percentage": 5.0
    }}
  ],
  "assignments": [
    {{
      "assignment_id": "assign_{module_number}_1",
      "title": "Assignment title",
      "description": "Assignment description",
      "type": "Essay",
      "topics_covered": ["topic_{module_number}_1"],
      "requirements": ["requirement1", "requirement2"],
      "deliverables": ["deliverable1"],
      "grading_criteria": [
        {{"criterion": "Content Quality", "points": "40"}},
        {{"criterion": "Analysis", "points": "30"}},
        {{"criterion": "Organization", "points": "30"}}
      ],
      "estimated_work_hours": 3.0,
      "weight_percentage": 10.0
    }}
  ]
}}

IMPORTANT: 
- Calculate total_reading_time_minutes for each topic by summing subtopic reading times
- Each subtopic must have reading time between {min_reading_time}-{max_reading_time} minutes
- Vary reading times across subtopics (don't use same time for all)"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a curriculum designer. Return only valid JSON. Ensure all reading times are between 5-10 minutes per subtopic."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )
        
        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()
        
        module_data = json.loads(content)
        
        # Calculate and validate reading times
        total_module_time = 0
        for topic in module_data.get("topics", []):
            topic_time = 0
            for subtopic in topic.get("subtopics", []):
                content_spec = subtopic.get("content_specification", {})
                reading_time = content_spec.get("estimated_reading_time_minutes", avg_reading_time)
                
                # Validate reading time is between 5-10 minutes
                if reading_time < min_reading_time or reading_time > max_reading_time:
                    logger.warning(f"Adjusting reading time from {reading_time} to {avg_reading_time} minutes")
                    content_spec["estimated_reading_time_minutes"] = avg_reading_time
                    content_spec["word_count_estimate"] = avg_reading_time * words_per_minute
                    reading_time = avg_reading_time
                
                topic_time += reading_time
            
            topic["total_reading_time_minutes"] = topic_time
            total_module_time += topic_time
        
        module_data["total_reading_time_minutes"] = total_module_time
        
        logger.info(f"Successfully generated module {module_number} with {total_module_time} minutes of reading content")
        return module_data
        
    except Exception as e:
        logger.error(f"Error generating module {module_number}: {str(e)}")
        raise

def generate_exam(client: OpenAI, exam_type: str, semester_num: int, modules: List[Dict]) -> Optional[Dict]:
    """Generate exam specification"""
    logger.info(f"Generating {exam_type} exam for semester {semester_num}")
    
    module_ids = [m["module_id"] for m in modules]
    
    prompt = f"""Generate a {exam_type} examination.

EXAM CONTEXT:
- Type: {exam_type}
- Covers modules: {', '.join(module_ids)}
- Number of modules: {len(module_ids)}

Return ONLY valid JSON:
{{
  "exam_id": "{exam_type.lower()}_sem_{semester_num}",
  "title": "Semester {semester_num} {exam_type} Examination",
  "type": "{exam_type}",
  "coverage": {json.dumps(module_ids)},
  "format": "Comprehensive written examination",
  "question_types": {{"mcq": 30, "short_answer": 10, "essay": 3}},
  "total_questions": 43,
  "duration_minutes": 90,
  "difficulty_distribution": {{"easy": 15, "medium": 20, "hard": 8}},
  "topics_breakdown": [
    {{"module_id": "{module_ids[0]}", "weightage_percentage": {100 // len(module_ids)}}}
  ],
  "passing_score_percentage": 60,
  "weight_percentage": 20.0,
  "allowed_materials": [],
  "instructions": ["Read all questions carefully", "Manage your time wisely", "Answer all sections"]
}}"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an assessment designer. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        
        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()
        
        exam_data = json.loads(content)
        logger.info(f"Successfully generated {exam_type} exam")
        return exam_data
        
    except Exception as e:
        logger.error(f"Error generating {exam_type} exam: {str(e)}")
        raise

# ==================== ENDPOINTS ====================

@app.get("/")
async def root():
    logger.info("Root endpoint accessed")
    return {
        "message": "Course Management API",
        "version": "2.0.0",
        "endpoints": {
            "/generate-test": "POST - Generate a diagnostic test",
            "/generate-syllabus": "POST - Generate comprehensive course syllabus",
            "/health": "GET - Check API health"
        }
    }

@app.get("/health")
async def health_check():
    """Check API health"""
    api_key = os.getenv("OPENAI_API_KEY")
    logger.info("Health check performed")
    return {
        "status": "healthy",
        "openai_configured": bool(api_key)
    }

@app.post("/generate-test", response_model=DiagnosticTestResponse)
async def generate_diagnostic_test(request: DiagnosticTestRequest):
    """Generate a diagnostic test"""
    logger.info(f"Diagnostic test requested: {request.course_name}, {request.number_of_mcq} questions")
    
    try:
        client = get_openai_client()
        
        prompt = f"""Generate {request.number_of_mcq} diagnostic MCQ questions for:
- Course: {request.course_name}
- Subject: {request.subject}
- Grade: {request.target_grade_level}

Return ONLY valid JSON:
{{
  "questions": [
    {{
      "question_number": 1,
      "question": "Question text",
      "options": [
        {{"option": "A", "text": "Option A"}},
        {{"option": "B", "text": "Option B"}},
        {{"option": "C", "text": "Option C"}},
        {{"option": "D", "text": "Option D"}}
      ],
      "correct_answer": "A",
      "explanation": "Explanation"
    }}
  ]
}}"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are an assessment creator. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=4000
        )

        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        content = content.strip()

        test_data = json.loads(content)
        logger.info("Diagnostic test generated successfully")

        return DiagnosticTestResponse(
            course_name=request.course_name,
            subject=request.subject,
            target_grade_level=request.target_grade_level,
            total_questions=request.number_of_mcq,
            questions=test_data["questions"]
        )

    except json.JSONDecodeError as e:
        logger.error(f"JSON parse error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to parse response: {str(e)}")
    except Exception as e:
        logger.error(f"Error generating test: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/generate-syllabus", response_model=SyllabusResponse)
async def generate_syllabus(request: SyllabusRequest):
    """Generate comprehensive course syllabus with chunked generation"""
    logger.info(f"Syllabus generation started: {request.course_name}")
    logger.info(f"Parameters: {request.total_modules} modules, {request.semester_count} semesters, "
                f"{request.quizzes_per_module} quizzes/module")
    
    try:
        client = get_openai_client()
        
        # Step 1: Generate course metadata
        logger.info("Step 1: Generating course metadata")
        metadata_prompt = f"""Generate course metadata for {request.course_name} ({request.subject}, {request.target_grade_level}).

Return ONLY valid JSON:
{{
  "syllabus_id": "syllabus_{request.course_name.lower().replace(' ', '_')}_{datetime.now().strftime('%Y%m%d')}",
  "course_metadata": {{
    "course_code": "CODE-101",
    "credits": 4,
    "prerequisites": [],
    "corequisites": [],
    "required_materials": ["Textbook", "Notebook"],
    "recommended_materials": ["Reference book"],
    "learning_platform": "Online"
  }},
  "course_description": "Comprehensive course description for {request.course_name}",
  "course_objectives": ["Objective 1", "Objective 2", "Objective 3"]
}}"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a curriculum designer. Return only valid JSON."},
                {"role": "user", "content": metadata_prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )
        
        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content[7:]
        if content.startswith("```"):
            content = content[3:]
        if content.endswith("```"):
            content = content[:-3]
        
        base_data = json.loads(content.strip())
        logger.info("Course metadata generated successfully")
        
        # Step 2: Generate modules in chunks
        logger.info(f"Step 2: Generating {request.total_modules} modules")
        all_modules = []
        total_content_time = 0
        
        for i in range(1, request.total_modules + 1):
            module = generate_module(client, request, i, request.total_modules)
            all_modules.append(module)
            total_content_time += module.get("total_reading_time_minutes", 0)
        
        logger.info(f"All {len(all_modules)} modules generated with {total_content_time} minutes total reading time")
        
        # Step 3: Organize modules into semesters
        logger.info("Step 3: Organizing modules into semesters")
        modules_per_semester = request.total_modules // request.semester_count
        remainder = request.total_modules % request.semester_count
        
        semesters = []
        module_index = 0
        
        for sem_num in range(1, request.semester_count + 1):
            # Calculate modules for this semester
            num_modules = modules_per_semester + (1 if sem_num <= remainder else 0)
            semester_modules = all_modules[module_index:module_index + num_modules]
            module_index += num_modules
            
            logger.info(f"Semester {sem_num}: {num_modules} modules")
            
            # Generate exams if enabled
            midterm = None
            final = None
            
            if request.midterm_examination == ExamStatus.ENABLED:
                midterm = generate_exam(client, "Midterm", sem_num, semester_modules)
            
            if request.final_examination == ExamStatus.ENABLED:
                final = generate_exam(client, "Final", sem_num, semester_modules)
            
            semester = {
                "semester_id": f"sem_{sem_num}",
                "semester_number": sem_num,
                "title": f"Semester {sem_num}",
                "description": f"Semester {sem_num} of {request.course_name}",
                "total_weeks": 12,
                "semester_learning_outcomes": [
                    f"Master core concepts covered in semester {sem_num}",
                    f"Apply learned skills to practical scenarios"
                ],
                "modules": semester_modules,
                "midterm_exam": midterm,
                "final_exam": final
            }
            semesters.append(semester)
        
        # Step 4: Generate assessment summary
        logger.info("Step 4: Generating assessment summary")
        total_quizzes = request.total_modules * request.quizzes_per_module
        
        grading_percentages = {
            "quizzes_percentage": 30.0,
            "assignments_percentage": 25.0,
            "midterm_percentage": 20.0 if request.midterm_examination == ExamStatus.ENABLED else 0.0,
            "final_percentage": 25.0 if request.final_examination == ExamStatus.ENABLED else 0.0,
            "participation_percentage": 0.0,
            "projects_percentage": 0.0
        }
        
        # Normalize percentages to 100%
        total_percentage = sum(grading_percentages.values())
        if total_percentage > 0:
            for key in grading_percentages:
                grading_percentages[key] = round((grading_percentages[key] / total_percentage) * 100, 1)
        
        assessment_summary = {
            "total_quizzes": total_quizzes,
            "total_assignments": request.total_modules,
            "midterm_enabled": request.midterm_examination == ExamStatus.ENABLED,
            "final_enabled": request.final_examination == ExamStatus.ENABLED,
            "grading_breakdown": grading_percentages,
            "grading_scale": {
                "A": "90-100",
                "B": "80-89",
                "C": "70-79",
                "D": "60-69",
                "F": "0-59"
            }
        }
        
        # Step 5: Assemble final syllabus
        logger.info("Step 5: Assembling final syllabus")
        syllabus_data = {
            **base_data,
            "course_name": request.course_name,
            "subject": request.subject,
            "target_grade_level": request.target_grade_level,
            "course_length": request.course_length,
            "total_modules": request.total_modules,
            "total_semesters": request.semester_count,
            "semesters": semesters,
            "assessment_summary": assessment_summary,
            "total_content_reading_time_minutes": total_content_time
        }
        
        syllabus_response = SyllabusResponse(**syllabus_data)
        logger.info(f"Syllabus generated successfully - Total reading time: {total_content_time} minutes ({total_content_time/60:.1f} hours)")
        
        return syllabus_response

    except json.JSONDecodeError as e:
        logger.error(f"JSON parse error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to parse response: {str(e)}")
    except Exception as e:
        logger.error(f"Error generating syllabus: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    logger.info("Starting Course Management API")
    uvicorn.run(app, host="0.0.0.0", port=8000)